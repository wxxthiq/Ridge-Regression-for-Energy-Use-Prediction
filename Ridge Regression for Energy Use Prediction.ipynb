{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Brief: Fundamentals of Numpy and Pandas for Machine Learning  \n",
    "\n",
    "## Deadline: 01 November 2024, 14:00 GMT\n",
    "\n",
    "## Number of marks available: 10\n",
    "\n",
    "In this practical, we will practice using numpy and pandas to implement the fundamentals of machine learning experiments such as data splitting and model training and evaluation. \n",
    "\n",
    "### Please READ the whole assignment first, before starting to work on it.\n",
    "\n",
    "### How and what to submit\n",
    "\n",
    "A. A **Jupyter Notebook** with the code in all the cells executed and outputs displayed.\n",
    "\n",
    "B. Name your Notebook **COM61011_AssignmentA1_XXXXXX.ipynb** where XXXXXX is your username such as such as abc18de. Example: `COM61011_AssignmentA1_abc18de.ipynb`\n",
    "\n",
    "C. Upload the Jupyter Notebook in B to Blackboard under the **Group A: Computing Assignment 1** submission area before the deadline. **There are two submissions: please pay close attention to submit to the right place!**\n",
    "\n",
    "D. **NO DATA UPLOAD**: Please do not upload the data files used in this Notebook. We have a copy already. \n",
    "\n",
    "\n",
    "### Assessment Criteria \n",
    "\n",
    "* Being able to use numpy and pandas to preprocess a dataset.\n",
    "\n",
    "* Being able to follow the steps involved in an end-to-end project in machine learning.\n",
    "\n",
    "* Be able to implement, from scratch, a linear model and train it using gradient descent.\n",
    "\n",
    "\n",
    "### Code quality and use of Python libraries\n",
    "When writing your code, you will find out that there are operations that are repeated at least twice. If your code is unreadable, we may not award marks for that section. Make sure to check the following:\n",
    "\n",
    "* Did you include Python functions to solve the question and avoid repeating code? \n",
    "* Did you comment your code to make it readable to others?\n",
    "\n",
    "**DO NOT USE scikit-learn for the questions on this assignment. You are meant to write Python code from scratch. Using scikit-learn for the questions on this assignment will give ZERO marks. No excuse will be accepted.**\n",
    "\n",
    "Furthermore, please try to avoid using any imports apart from the ones already provided in the Notebook. You can easily install all recommended modules for this assignment by running the following command in your terminal: `python -m pip install -r requirements.txt`\n",
    "\n",
    "\n",
    "### Late submissions\n",
    "\n",
    "We follow Department's guidelines about late submissions, i.e., a deduction of 10% of the mark each 24 hours the work is late after the deadline. NO late submission will be marked one week after the deadline. Please read [this link](https://wiki.cs.manchester.ac.uk/index.php/UGHandbook23:Main#Late_Submission_of_Coursework_Penalty). \n",
    "\n",
    "### Use of unfair means \n",
    "\n",
    "**Any form of unfair means is treated as a serious academic offence and action may be taken under the Discipline Regulations.** Please carefully read [what constitutes Unfair Means](https://documents.manchester.ac.uk/display.aspx?DocID=2870) if not sure. If you still have questions, please ask your Personal tutor or the Lecturers.\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: regularised ridge regression and gradient descent\n",
    "\n",
    "Regularisation is a technique commonly used in Machine Learning to prevent overfitting. It consists on adding terms to the objective function such that the optimisation procedure avoids solutions that just learn the training data. Popular techniques for regularisation in Supervised Learning include [Lasso Regression](https://en.wikipedia.org/wiki/Lasso_(statistics)), [Ridge Regression](https://en.wikipedia.org/wiki/Tikhonov_regularization) and the [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization). \n",
    "\n",
    "Here we will build a Ridge Regression model, and implement equations to optimise the objective function using the update rules for gradient descent. You will use those update rules for making predictions on an energy use dataset.\n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "Let us start with a data set for training $\\mathcal{D} = \\{\\mathbf{y}, \\mathbf{X}\\}$, where the vector $\\mathbf{y}=[y_1, \\cdots, y_N]^{\\top}$ and $\\mathbf{X}$ is the design matrix from Lab 3, this is, \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{X} = \n",
    "                \\begin{bmatrix}\n",
    "                        1 & x_{1,1} & \\cdots & x_{1, D}\\\\\n",
    "                        1 & x_{2,1} & \\cdots & x_{2, D}\\\\\n",
    "                   \\vdots &  \\vdots\\\\\n",
    "                        1 & x_{N,1} & \\cdots & x_{N, D}\n",
    "                \\end{bmatrix}\n",
    "               = \n",
    "               \\begin{bmatrix}\n",
    "                      \\mathbf{x}_1^{\\top}\\\\\n",
    "                       \\mathbf{x}_2^{\\top}\\\\\n",
    "                          \\vdots\\\\\n",
    "                        \\mathbf{x}_N^{\\top}\n",
    "                \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "Our predictive model is going to be a linear model\n",
    "\n",
    "$$ f(\\mathbf{x}_i) = \\mathbf{w}^{\\top}\\mathbf{x}_i,$$\n",
    "\n",
    "where $\\mathbf{w} = [w_0\\; w_1\\; \\cdots \\; w_D]^{\\top}$.\n",
    "\n",
    "The **objective function** we are going to use has the following form\n",
    "\n",
    "$$ E(\\mathbf{w}, \\lambda) = \\frac{1}{N}\\sum_{n=1}^N (y_n - f(\\mathbf{x}_n))^2 + \\frac{\\lambda}{2}\\sum_{j=0}^D w_j^2,$$\n",
    "\n",
    "where $\\lambda>0$ is known as the *regularisation* parameter.\n",
    "\n",
    "This objective function was studied in Lecture 3. \n",
    "\n",
    "The first term on the rhs is what we call the \"fitting\" term whereas the second term in the expression is the regularisation term. Given $\\lambda$, the two terms in the expression have different purposes. The first term is looking for a value of $\\mathbf{w}$ that leads the squared-errors to zero. While doing this, $\\mathbf{w}$ can take any value and lead to a solution that it is only good for the training data but perhaps not for the test data. The second term is regularising the behavior of the first term by driving the $\\mathbf{w}$ towards zero. By doing this, it restricts the possible set of values that $\\mathbf{w}$ might take according to the first term. The value that we use for $\\lambda$ will allow a compromise between a value of $\\mathbf{w}$ that exactly fits the data (first term) or a value of $\\mathbf{w}$ that does not grow too much (second term).\n",
    "\n",
    "This type of regularisation has different names: ridge regression, Tikhonov regularisation or $\\ell_2$ norm regularisation. \n",
    "\n",
    "### Optimising the objective function with respect to $\\mathbf{w}$\n",
    "\n",
    "There are two ways we can optimise the objective function with respect to $\\mathbf{w}$. The first one leads to a closed form expression for $\\mathbf{w}$ and the second one using an iterative optimisation procedure that updates the value of $\\mathbf{w}$ at each iteration by using the gradient of the objective function with respect to $\\mathbf{w}$,\n",
    "$$\n",
    "\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\eta \\frac{d E(\\mathbf{w}, \\lambda)}{d\\mathbf{w}},\n",
    "$$\n",
    "where $\\eta$ is the *learning rate* parameter and $\\frac{d E(\\mathbf{w}, \\lambda)}{d\\mathbf{w}}$ is the gradient of the objective function.\n",
    "\n",
    "It can be shown (this is a question in the Exercise Sheet 3) that a closed-form expression for the optimal $\\mathbf{w}_*$ is given as\n",
    "\n",
    "\\begin{align*}            \n",
    "            \\mathbf{w}_*& = \\left(\\mathbf{X}^{\\top}\\mathbf{X} + \\frac{\\lambda N}   \n",
    "                                     {2}\\mathbf{I}\\right)^{-1}\\mathbf{X}^{\\top}\\mathbf{y}.\n",
    "\\end{align*}\n",
    "\n",
    "Alternatively, we can find an update equation for $\\mathbf{w}_{\\text{new}}$ using gradient descent leading to:\n",
    "\n",
    "\\begin{align*}\n",
    "   \\mathbf{w}_{\\text{new}} & = \\mathbf{w}_{\\text{old}} - \\eta \\frac{d E(\\mathbf{w}, \\lambda)}\n",
    "                              {d\\mathbf{w}},\\\\\n",
    "                           & = \\mathbf{w}_{\\text{old}} +  \\frac{2\\eta}{N}\\sum_{n=1}^N   \n",
    "                               \\left(y_n - \\mathbf{x}_n^{\\top}\\mathbf{w}_{\\text{old}}\\right)\\mathbf{x}_n  \n",
    "                       - \\eta\\lambda\\mathbf{w}_{\\text{old}}\\\\\n",
    "                           & = (1 - \\eta\\lambda)\\mathbf{w}_{\\text{old}} + \\frac{2\\eta}\n",
    "                               {N}\\sum_{n=1}^N   \n",
    "                               \\left(y_n - \\mathbf{x}_n^{\\top}\\mathbf{w}_{\\text{old}}\\right)\\mathbf{x}_n\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-set up: imports and random seed\n",
    "\n",
    "**Important: set a random seed below that corresponds to the last five digits of your student ID number.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request, os, zipfile\n",
    "\n",
    "rng = np.random.default_rng(60670) # replace xxxxx with the last 5 digits of your student ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataset that we will be using is from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), a popular repository for open source datasets for educational and research purposes. We are going to use ridge regression to predict the energy use of appliances in a low energy building. The dataset is available [here](https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction) with an [accompanying paper](https://www.sciencedirect.com/science/article/pii/S0378778816308970?via%3Dihub).\n",
    "\n",
    "We can view some of the rows in the dataset with the `.sample()` method, or print the first few rows with the `.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "if not os.path.exists('./appliances.zip'):\n",
    "    durl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv\"\n",
    "    save_path = \"./appliances.zip\"\n",
    "    urllib.request.urlretrieve(durl, save_path)\n",
    "\n",
    "# Unzip the data\n",
    "zip = zipfile.ZipFile('./appliances.zip', 'r')\n",
    "for name in zip.namelist():\n",
    "    zip.extract(name, '.')\n",
    "\n",
    "# Read the data into a pandas dataframe\n",
    "energy_appliances_full = pd.read_csv('./energydata_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-11 17:00:00</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.89</td>\n",
       "      <td>47.596667</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.033333</td>\n",
       "      <td>45.53</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>733.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>13.275433</td>\n",
       "      <td>13.275433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-11 17:10:00</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.693333</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.722500</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>45.56</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>733.6</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>5.2</td>\n",
       "      <td>18.606195</td>\n",
       "      <td>18.606195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-11 17:20:00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.626667</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.926667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.50</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>733.7</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>5.1</td>\n",
       "      <td>28.642668</td>\n",
       "      <td>28.642668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-11 17:30:00</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.590000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.40</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>733.8</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>45.410389</td>\n",
       "      <td>45.410389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 17:40:00</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.530000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.40</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>733.9</td>\n",
       "      <td>92.0</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>4.9</td>\n",
       "      <td>10.084097</td>\n",
       "      <td>10.084097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  Appliances  lights     T1       RH_1    T2       RH_2  \\\n",
       "0  2016-01-11 17:00:00          60      30  19.89  47.596667  19.2  44.790000   \n",
       "1  2016-01-11 17:10:00          60      30  19.89  46.693333  19.2  44.722500   \n",
       "2  2016-01-11 17:20:00          50      30  19.89  46.300000  19.2  44.626667   \n",
       "3  2016-01-11 17:30:00          50      40  19.89  46.066667  19.2  44.590000   \n",
       "4  2016-01-11 17:40:00          60      40  19.89  46.333333  19.2  44.530000   \n",
       "\n",
       "      T3       RH_3         T4  ...         T9   RH_9     T_out  Press_mm_hg  \\\n",
       "0  19.79  44.730000  19.000000  ...  17.033333  45.53  6.600000        733.5   \n",
       "1  19.79  44.790000  19.000000  ...  17.066667  45.56  6.483333        733.6   \n",
       "2  19.79  44.933333  18.926667  ...  17.000000  45.50  6.366667        733.7   \n",
       "3  19.79  45.000000  18.890000  ...  17.000000  45.40  6.250000        733.8   \n",
       "4  19.79  45.000000  18.890000  ...  17.000000  45.40  6.133333        733.9   \n",
       "\n",
       "   RH_out  Windspeed  Visibility  Tdewpoint        rv1        rv2  \n",
       "0    92.0   7.000000   63.000000        5.3  13.275433  13.275433  \n",
       "1    92.0   6.666667   59.166667        5.2  18.606195  18.606195  \n",
       "2    92.0   6.333333   55.333333        5.1  28.642668  28.642668  \n",
       "3    92.0   6.000000   51.500000        5.0  45.410389  45.410389  \n",
       "4    92.0   5.666667   47.666667        4.9  10.084097  10.084097  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View 5 random rows of the data\n",
    "energy_appliances_full.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, the column headings are are printed below:\n",
    "\n",
    "| Data variables | Units |\n",
    "|----------------|-------|\n",
    "Date time stamp year-month-day | hour:min:s\n",
    "| Appliances energy consumption | Wh |\n",
    "| Light energy consumption | Wh | \n",
    "| T1, Temperature in kitchen area | ◦C | \n",
    "| RH1, Humidity in kitchen area | % | \n",
    "| T2, Temperature in living room area | ◦C | \n",
    "| RH2, Humidity in living room area | % | \n",
    "T3, Temperature in laundry room area | ◦C \n",
    "RH3, Humidity in laundry room area | % \n",
    "T4, Temperature in office room | ◦C \n",
    "RH4, Humidity in office room | % \n",
    "T5, Temperature in bathroom | ◦C \n",
    "RH5, Humidity in bathroom | % \n",
    "T6, Temperature outside the building (north side) | ◦C \n",
    "RH6, Humidity outside the building (north side) | % \n",
    "T7, Temperature in ironing room | ◦C \n",
    "RH7, Humidity in ironing room | % \n",
    "T8, Temperature in teenager room 2 | ◦C \n",
    "RH8, Humidity in teenager room 2 | % \n",
    "T9, Temperature in parents room | ◦C \n",
    "RH9, Humidity in parents room | % \n",
    "To, Temperature outside (from Chièvres weather station) | ◦C \n",
    "Pressure (from Chièvres weather station) | mm Hg \n",
    "RHo, Humidity outside (from Chièvres weather station) | % \n",
    "Windspeed (from Chièvres weather station) | m/s \n",
    "Visibility (from Chièvres weather station) | km \n",
    "Tdewpoint (from Chièvres weather station) | ◦C \n",
    "Random Variable 1 (RV 1) | Non dimensional\n",
    "Random Variable 2 (RV 2) | Non dimensional "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first perform some minor data cleaning. \n",
    "\n",
    "We can't use `datetime` directly, since it's not a number, and encoding it as a continuous variable also creates issues. So let's try to extract some information from it that we can use. In this case, we'll use a binary variable to encode whether the day is a weekday or weekend. These are called categorical or dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>...</th>\n",
       "      <th>RH_8</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>is_weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13354</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>42.366667</td>\n",
       "      <td>20.2300</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>23.230000</td>\n",
       "      <td>38.826667</td>\n",
       "      <td>22.166667</td>\n",
       "      <td>40.130000</td>\n",
       "      <td>...</td>\n",
       "      <td>39.400000</td>\n",
       "      <td>20.50</td>\n",
       "      <td>39.590000</td>\n",
       "      <td>9.433333</td>\n",
       "      <td>750.900000</td>\n",
       "      <td>79.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7686</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>20.890000</td>\n",
       "      <td>37.200000</td>\n",
       "      <td>18.2225</td>\n",
       "      <td>39.900000</td>\n",
       "      <td>21.365000</td>\n",
       "      <td>37.900000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>36.290000</td>\n",
       "      <td>...</td>\n",
       "      <td>43.060000</td>\n",
       "      <td>18.39</td>\n",
       "      <td>40.067500</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>740.300000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19162</th>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>25.356667</td>\n",
       "      <td>47.026667</td>\n",
       "      <td>24.2900</td>\n",
       "      <td>46.390000</td>\n",
       "      <td>25.926667</td>\n",
       "      <td>42.530000</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>...</td>\n",
       "      <td>46.156667</td>\n",
       "      <td>23.00</td>\n",
       "      <td>41.970909</td>\n",
       "      <td>14.366667</td>\n",
       "      <td>758.100000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>9.766667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18307</th>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>23.890000</td>\n",
       "      <td>40.933333</td>\n",
       "      <td>23.5375</td>\n",
       "      <td>38.443750</td>\n",
       "      <td>24.640000</td>\n",
       "      <td>38.134000</td>\n",
       "      <td>23.790000</td>\n",
       "      <td>39.663333</td>\n",
       "      <td>...</td>\n",
       "      <td>42.326667</td>\n",
       "      <td>22.60</td>\n",
       "      <td>39.077143</td>\n",
       "      <td>14.850000</td>\n",
       "      <td>756.622222</td>\n",
       "      <td>68.750000</td>\n",
       "      <td>3.277778</td>\n",
       "      <td>38.472222</td>\n",
       "      <td>9.158333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>18.290000</td>\n",
       "      <td>37.826667</td>\n",
       "      <td>16.7900</td>\n",
       "      <td>37.933333</td>\n",
       "      <td>18.533333</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>36.966667</td>\n",
       "      <td>...</td>\n",
       "      <td>42.435714</td>\n",
       "      <td>16.10</td>\n",
       "      <td>37.890000</td>\n",
       "      <td>-2.300000</td>\n",
       "      <td>759.500000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>-4.150000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Appliances  lights         T1       RH_1       T2       RH_2  \\\n",
       "13354          60       0  22.200000  42.366667  20.2300  44.933333   \n",
       "7686           50       0  20.890000  37.200000  18.2225  39.900000   \n",
       "19162         120       0  25.356667  47.026667  24.2900  46.390000   \n",
       "18307         140       0  23.890000  40.933333  23.5375  38.443750   \n",
       "1245           60      10  18.290000  37.826667  16.7900  37.933333   \n",
       "\n",
       "              T3       RH_3         T4       RH_4  ...       RH_8     T9  \\\n",
       "13354  23.230000  38.826667  22.166667  40.130000  ...  39.400000  20.50   \n",
       "7686   21.365000  37.900000  19.000000  36.290000  ...  43.060000  18.39   \n",
       "19162  25.926667  42.530000  24.200000  44.933333  ...  46.156667  23.00   \n",
       "18307  24.640000  38.134000  23.790000  39.663333  ...  42.326667  22.60   \n",
       "1245   18.533333  38.500000  17.100000  36.966667  ...  42.435714  16.10   \n",
       "\n",
       "            RH_9      T_out  Press_mm_hg     RH_out  Windspeed  Visibility  \\\n",
       "13354  39.590000   9.433333   750.900000  79.333333   1.000000   40.000000   \n",
       "7686   40.067500   0.600000   740.300000  97.000000   2.000000   61.000000   \n",
       "19162  41.970909  14.366667   758.100000  74.000000   6.333333   40.000000   \n",
       "18307  39.077143  14.850000   756.622222  68.750000   3.277778   38.472222   \n",
       "1245   37.890000  -2.300000   759.500000  87.000000   3.000000   61.500000   \n",
       "\n",
       "       Tdewpoint  is_weekday  \n",
       "13354   5.933333           1  \n",
       "7686    0.200000           0  \n",
       "19162   9.766667           1  \n",
       "18307   9.158333           1  \n",
       "1245   -4.150000           1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the datetime from the date column and save in a separate dataframe. We'll have to treat this specially in order\n",
    "# to use it in regression.\n",
    "datetime = pd.to_datetime(energy_appliances_full['date'])\n",
    "\n",
    "# Drop the date and last two columns (rv1 and rv2) as they are not useful for regression.\n",
    "energy_appliances = energy_appliances_full.drop(['date', 'rv1', 'rv2'], axis=1)\n",
    "\n",
    "# Create a new column with a binary dummy variable: 1 if the day is a weekday, 0 if it is a weekend\n",
    "energy_appliances['is_weekday'] = (datetime.dt.dayofweek < 5).astype(int)\n",
    "\n",
    "energy_appliances.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset\n",
    "\n",
    "Before designing any machine learning model, we need to set aside the test data. We will use the remaining training data for fitting the model. *It is important to remember that the test data has to be set aside before preprocessing*. \n",
    "\n",
    "Any preprocessing that you do has to be calibrated *only* on the training data, and several key statistics from this preprocessing need to be saved for the test stage. Separating the dataset into training and test before any preprocessing has happened helps us to recreate the real world scenario where we will deploy our system and for which the data will come without any preprocessing.\n",
    "\n",
    "Furthermore, we are going to use *hold-out validation* for validating our predictive model, so we need to further separate the training data into a training set and a validation set.\n",
    "\n",
    "In this step, we will first **shuffle the data**, then split the dataset into a training set, a validation set and a test set: \n",
    "- The training set will have 70% of the total observations,\n",
    "- The validation set will have 15% of the total observations,\n",
    "- The test set will have the remaining 15%. \n",
    "\n",
    "If this doesn't run, check you have correctly initialised the `default_rng()` object with a random seed in **Pre-set up** above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = energy_appliances.shape[0]\n",
    "index = np.arange(ndata)\n",
    "rng.shuffle(index)                        # Permute the indexes \n",
    "Ntrain = np.int64(np.round(0.70*ndata))   # We compute Ntrain, the number of training instances\n",
    "Nval = np.int64(np.round(0.15*ndata))     # We compute Nval, the number of validation instances   \n",
    "Ntest = ndata - Ntrain - Nval             # We compute Ntest, the number of test instances\n",
    "\n",
    "# Split the data into training, validation and test sets\n",
    "# We note that the first column (index 0) is the target variable, what we're trying to predict\n",
    "data_training = energy_appliances.iloc[index[0:Ntrain], 1:].copy() # Select the training data\n",
    "labels_training = energy_appliances.iloc[index[0:Ntrain], 0].copy() # Select the training labels\n",
    "\n",
    "data_val = energy_appliances.iloc[index[Ntrain:Ntrain+Nval], 1:].copy() # Select the validation data\n",
    "labels_val = energy_appliances.iloc[index[Ntrain:Ntrain+Nval], 0].copy() # Select the validation labels\n",
    "\n",
    "data_test = energy_appliances.iloc[index[Ntrain+Nval:ndata], 1:].copy() # Select the test data\n",
    "labels_test = energy_appliances.iloc[index[Ntrain+Nval:ndata], 0].copy() # Select the test labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "It's important to preprocess the data before fitting a model. This includes:\n",
    "- Handling missing values\n",
    "- Scale the data\n",
    "- Encoding categorical or time variables\n",
    "\n",
    "We have already completed the encoding of the datetime variable above, and there are no missing values in this dataset. The only thing left to do is scale the data. Since most of our data is normally distributed, we will use a process called standardization, which scales the data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Note that we *don't* standardize our categorical variable (`is_weekday`), since it's not a continuous variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>T5</th>\n",
       "      <th>...</th>\n",
       "      <th>RH_8</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>is_weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>13814.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.086185e-18</td>\n",
       "      <td>3.161540e-15</td>\n",
       "      <td>1.090967e-15</td>\n",
       "      <td>-1.682485e-15</td>\n",
       "      <td>3.075898e-16</td>\n",
       "      <td>7.455709e-16</td>\n",
       "      <td>-2.619143e-15</td>\n",
       "      <td>6.383260e-16</td>\n",
       "      <td>2.317211e-16</td>\n",
       "      <td>-5.889470e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.055323e-16</td>\n",
       "      <td>-1.489342e-15</td>\n",
       "      <td>5.892042e-16</td>\n",
       "      <td>-1.080165e-17</td>\n",
       "      <td>9.938545e-15</td>\n",
       "      <td>1.327060e-16</td>\n",
       "      <td>4.732151e-17</td>\n",
       "      <td>5.802028e-16</td>\n",
       "      <td>1.530234e-17</td>\n",
       "      <td>0.721225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.448413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.808992e-01</td>\n",
       "      <td>-3.049638e+00</td>\n",
       "      <td>-3.323732e+00</td>\n",
       "      <td>-1.935738e+00</td>\n",
       "      <td>-4.919038e+00</td>\n",
       "      <td>-2.526764e+00</td>\n",
       "      <td>-3.219803e+00</td>\n",
       "      <td>-2.828662e+00</td>\n",
       "      <td>-2.513547e+00</td>\n",
       "      <td>-2.310701e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.553231e+00</td>\n",
       "      <td>-2.281573e+00</td>\n",
       "      <td>-2.995545e+00</td>\n",
       "      <td>-2.336856e+00</td>\n",
       "      <td>-3.536678e+00</td>\n",
       "      <td>-3.766659e+00</td>\n",
       "      <td>-1.648430e+00</td>\n",
       "      <td>-3.166967e+00</td>\n",
       "      <td>-2.466489e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-4.808992e-01</td>\n",
       "      <td>-5.745242e-01</td>\n",
       "      <td>-7.345347e-01</td>\n",
       "      <td>-7.046290e-01</td>\n",
       "      <td>-6.231181e-01</td>\n",
       "      <td>-7.341144e-01</td>\n",
       "      <td>-7.211434e-01</td>\n",
       "      <td>-6.320225e-01</td>\n",
       "      <td>-8.022633e-01</td>\n",
       "      <td>-7.058743e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.377356e-01</td>\n",
       "      <td>-7.368942e-01</td>\n",
       "      <td>-7.346298e-01</td>\n",
       "      <td>-7.061524e-01</td>\n",
       "      <td>-6.202079e-01</td>\n",
       "      <td>-6.403990e-01</td>\n",
       "      <td>-8.328991e-01</td>\n",
       "      <td>-7.944145e-01</td>\n",
       "      <td>-6.815195e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-4.808992e-01</td>\n",
       "      <td>-5.082242e-02</td>\n",
       "      <td>-1.541308e-01</td>\n",
       "      <td>-1.508589e-01</td>\n",
       "      <td>1.144242e-02</td>\n",
       "      <td>-7.997221e-02</td>\n",
       "      <td>-2.111710e-01</td>\n",
       "      <td>-9.761627e-02</td>\n",
       "      <td>-1.441913e-01</td>\n",
       "      <td>-1.084771e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.045122e-01</td>\n",
       "      <td>-4.650711e-02</td>\n",
       "      <td>-1.623583e-01</td>\n",
       "      <td>-9.346038e-02</td>\n",
       "      <td>7.290367e-02</td>\n",
       "      <td>2.704897e-01</td>\n",
       "      <td>-1.532902e-01</td>\n",
       "      <td>1.376599e-01</td>\n",
       "      <td>-7.066338e-02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-4.808992e-01</td>\n",
       "      <td>5.726320e-01</td>\n",
       "      <td>7.051387e-01</td>\n",
       "      <td>5.356330e-01</td>\n",
       "      <td>6.979858e-01</td>\n",
       "      <td>5.142486e-01</td>\n",
       "      <td>7.811246e-01</td>\n",
       "      <td>6.138326e-01</td>\n",
       "      <td>7.282440e-01</td>\n",
       "      <td>5.667628e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>6.970607e-01</td>\n",
       "      <td>5.544774e-01</td>\n",
       "      <td>6.827683e-01</td>\n",
       "      <td>5.726458e-01</td>\n",
       "      <td>7.322598e-01</td>\n",
       "      <td>7.990301e-01</td>\n",
       "      <td>5.942795e-01</td>\n",
       "      <td>1.376599e-01</td>\n",
       "      <td>6.671239e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.326295e+00</td>\n",
       "      <td>2.854475e+00</td>\n",
       "      <td>4.853538e+00</td>\n",
       "      <td>4.360156e+00</td>\n",
       "      <td>3.832134e+00</td>\n",
       "      <td>3.483355e+00</td>\n",
       "      <td>3.353515e+00</td>\n",
       "      <td>2.630151e+00</td>\n",
       "      <td>2.771225e+00</td>\n",
       "      <td>3.370004e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.021431e+00</td>\n",
       "      <td>2.491535e+00</td>\n",
       "      <td>2.838244e+00</td>\n",
       "      <td>3.526135e+00</td>\n",
       "      <td>2.264757e+00</td>\n",
       "      <td>1.361307e+00</td>\n",
       "      <td>4.060285e+00</td>\n",
       "      <td>2.340745e+00</td>\n",
       "      <td>2.793221e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             lights            T1          RH_1            T2          RH_2  \\\n",
       "count  1.381400e+04  1.381400e+04  1.381400e+04  1.381400e+04  1.381400e+04   \n",
       "mean   3.086185e-18  3.161540e-15  1.090967e-15 -1.682485e-15  3.075898e-16   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -4.808992e-01 -3.049638e+00 -3.323732e+00 -1.935738e+00 -4.919038e+00   \n",
       "25%   -4.808992e-01 -5.745242e-01 -7.345347e-01 -7.046290e-01 -6.231181e-01   \n",
       "50%   -4.808992e-01 -5.082242e-02 -1.541308e-01 -1.508589e-01  1.144242e-02   \n",
       "75%   -4.808992e-01  5.726320e-01  7.051387e-01  5.356330e-01  6.979858e-01   \n",
       "max    8.326295e+00  2.854475e+00  4.853538e+00  4.360156e+00  3.832134e+00   \n",
       "\n",
       "                 T3          RH_3            T4          RH_4            T5  \\\n",
       "count  1.381400e+04  1.381400e+04  1.381400e+04  1.381400e+04  1.381400e+04   \n",
       "mean   7.455709e-16 -2.619143e-15  6.383260e-16  2.317211e-16 -5.889470e-17   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -2.526764e+00 -3.219803e+00 -2.828662e+00 -2.513547e+00 -2.310701e+00   \n",
       "25%   -7.341144e-01 -7.211434e-01 -6.320225e-01 -8.022633e-01 -7.058743e-01   \n",
       "50%   -7.997221e-02 -2.111710e-01 -9.761627e-02 -1.441913e-01 -1.084771e-01   \n",
       "75%    5.142486e-01  7.811246e-01  6.138326e-01  7.282440e-01  5.667628e-01   \n",
       "max    3.483355e+00  3.353515e+00  2.630151e+00  2.771225e+00  3.370004e+00   \n",
       "\n",
       "       ...          RH_8            T9          RH_9         T_out  \\\n",
       "count  ...  1.381400e+04  1.381400e+04  1.381400e+04  1.381400e+04   \n",
       "mean   ... -3.055323e-16 -1.489342e-15  5.892042e-16 -1.080165e-17   \n",
       "std    ...  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min    ... -2.553231e+00 -2.281573e+00 -2.995545e+00 -2.336856e+00   \n",
       "25%    ... -7.377356e-01 -7.368942e-01 -7.346298e-01 -7.061524e-01   \n",
       "50%    ... -1.045122e-01 -4.650711e-02 -1.623583e-01 -9.346038e-02   \n",
       "75%    ...  6.970607e-01  5.544774e-01  6.827683e-01  5.726458e-01   \n",
       "max    ...  3.021431e+00  2.491535e+00  2.838244e+00  3.526135e+00   \n",
       "\n",
       "        Press_mm_hg        RH_out     Windspeed    Visibility     Tdewpoint  \\\n",
       "count  1.381400e+04  1.381400e+04  1.381400e+04  1.381400e+04  1.381400e+04   \n",
       "mean   9.938545e-15  1.327060e-16  4.732151e-17  5.802028e-16  1.530234e-17   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -3.536678e+00 -3.766659e+00 -1.648430e+00 -3.166967e+00 -2.466489e+00   \n",
       "25%   -6.202079e-01 -6.403990e-01 -8.328991e-01 -7.944145e-01 -6.815195e-01   \n",
       "50%    7.290367e-02  2.704897e-01 -1.532902e-01  1.376599e-01 -7.066338e-02   \n",
       "75%    7.322598e-01  7.990301e-01  5.942795e-01  1.376599e-01  6.671239e-01   \n",
       "max    2.264757e+00  1.361307e+00  4.060285e+00  2.340745e+00  2.793221e+00   \n",
       "\n",
       "         is_weekday  \n",
       "count  13814.000000  \n",
       "mean       0.721225  \n",
       "std        0.448413  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize the data to zero mean and unit variance. Note we do NOT apply this to the labels OR to our binary variable!\n",
    "training_means = data_training.mean()\n",
    "training_stds = data_training.std()\n",
    "data_training_standardized = (data_training - training_means) / training_stds\n",
    "# Replace last column (is_weekend) with original binary value - we don't want to standardize this.\n",
    "data_training_standardized['is_weekday'] = data_training['is_weekday']\n",
    "\n",
    "# Let's use describe again: we should see that the mean is 0 (almost - some numerical overflow here) and the standard deviation \n",
    "# is 1 for each feature.\n",
    "data_training_standardized.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the preprocessing steps to the validation set as if it were new data: we use the values from the **training** data to standardize the **validation** data. This is important to ensure that the model generalizes well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val_standardized = (data_val - training_means) / training_stds\n",
    "data_val_standardized['is_weekday'] = data_val['is_weekday'] # don't forget to not standardize this column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and training a predictive model\n",
    "\n",
    "We have now split our data into training and validation data and applied relevant preprocessing steps. We are now in a good position to work on developing the prediction model and validating it. We will build a regularised ridge regression model and train it using gradient descent for iterative optimisation. \n",
    "\n",
    "We first organise the dataframes into the vector of targets $\\mathbf{y}$, call it `yTrain`, and the design matrix $\\mathbf{X}$, call it `XTrain`. We will augment `XTrain` with a column of ones: this is the design matrix. We repeat the process for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target vector and design matrix for training data\n",
    "yTrain = np.reshape(labels_training.values, (Ntrain,1)) # The training target labels as a column vector\n",
    "XTrain = np.concatenate((np.ones((Ntrain,1)), data_training_standardized.values), axis=1) # The standardised inputs with an additional column vector  \n",
    "\n",
    "# Do the same for val data\n",
    "yVal = np.reshape(labels_val.values, (Nval,1)) # The validation target labels as a column vector\n",
    "XVal = np.concatenate((np.ones((Nval,1)), data_val_standardized.values), axis=1) # The standardised inputs with an additional column vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal $\\mathbf{w}$ with stochastic gradient descent (5 marks)\n",
    "Now, we will use gradient descent to iteratively compute the value of $\\mathbf{w}_{\\text{new}}$. Instead of using all the training set in `XTrain` and `yTrain` to compute the gradient, use a subset of $S$ instances in `XTrain` and `yTrain`. This is sometimes called *minibatch gradient descent,* where $S$ is the size of the minibatch. \n",
    "\n",
    "You will need to find the best values for three parameters: $\\eta$, the learning rate, $S$, the number of datapoints in the minibatch, and $\\lambda$, the regularisation parameter. We can do this using a grid search over the validation set. You should complete the following tasks:\n",
    "\n",
    "* **Write the optimisation function:** Write a function, `mgd_optimiser`, that takes as input the training data and targets, the learning rate $\\eta$, the minibatch size $S$, the regularisation parameter $\\lambda$, and the number of iterations $T$. The function should return the optimal $\\mathbf{w}$ after the chosen number of iterations. \n",
    "\n",
    "* **Evaluate each set of hyperparameters:** For each value that you have of $\\lambda$, $\\eta$ and $S$ in your grid, use the training set to compute $\\mathbf{w}$ using your `mgd_optimiser` function, and then measure the RMSE using that $\\mathbf{w}$ over the validation data. For the minibatch gradient descent choose to stop the iterative procedure after $500$ iterations. \n",
    "\n",
    "* Choose the values of $\\lambda$, $\\eta$ and $S$ that lead to the lowest RMSE and save them. You will use them at the test stage.\n",
    "\n",
    "When writing these functions, you should avoid using for loops over individual features or samples; that is, you should be able to calculate the gradient, weight updates, and MSE in vectorised form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define the search space by creating a grid of values for the parameters $\\lambda$ and $\\eta$ using `np.logspace` and a grid of values for $S$ using `np.linspace`. Because we need to find three parameters, let's start with five values for each parameter in the grid (see if you can increase it - it may take some time to run). Make sure you understand the meaning of `np.logspace` and `np.linspace`. Notice that you can use negative values for `start` in `np.logspace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE GRID OF VALUES FOR LAMBDA, ETA AND S\n",
    "num = 5\n",
    "lambda_vector = np.logspace(-4, -1, num)\n",
    "eta_vector = np.logspace(-5, -2, num)\n",
    "S_vector = np.linspace(10, 200, num, dtype=int) # we want integer values for S (n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: Lambda = 0.0031622776601683794, Eta = 0.01, S = 152\n",
      "Best RMSE: 90.78578955204999\n"
     ]
    }
   ],
   "source": [
    "# TRAIN THE REGULARISED LINEAR MODEL AND COMPUTE THE RMSE FOR ALL VALUES OF LAMBDA, ETA AND S\n",
    "# note that 'lambda' is a reserved keyword in Python, so we use 'lmbd' instead\n",
    "\n",
    "def sgd_optimiser(X, y, lmbd, eta, S, max_iters=500):\n",
    "    # Randomly initialize weights \n",
    "    W = np.random.randn(X.shape[1], 1)\n",
    "    \n",
    "    # Perform gradient descent over n_samples (i.e., 500)\n",
    "    for iteration in range(max_iters):\n",
    "        # Shuffle the training data to ensure randomness and prevent bias in the mini-batches\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        # Process the data in mini-batches of size S\n",
    "        for i in range(0, X.shape[0], S):\n",
    "            # Get the current mini-batches from the shuffled data\n",
    "            X_batch = X_shuffled[i:i+S]\n",
    "            y_batch = y_shuffled[i:i+S]\n",
    "            \n",
    "            # Compute predictions for the mini-batch\n",
    "            y_pred = np.dot(X_batch, W)\n",
    "            \n",
    "            # Compute the gradient of the cost function\n",
    "            gradient = (2 / X_batch.shape[0]) * np.dot(X_batch.T, (y_pred - y_batch)) + lmbd * W\n",
    "            \n",
    "            # Update the weights using the learning rate and computed gradient\n",
    "            W = W - eta * gradient\n",
    "    \n",
    "    return W\n",
    "\n",
    "best_rmse = float('inf') # Initialize the best RMSE as infinity to track the lowest RMSE found\n",
    "optimal_params = None    # Initialize Tuple to store the optimal hyperparameters (i.e., lambda, learning rate and minibatch size)\n",
    "optimal_w = None         # Initialize 2D Numpy array to store the optimal weights\n",
    "\n",
    "# Iterate over all combinations of lambda, eta, and S\n",
    "for lmbd in lambda_vector:\n",
    "    for eta in eta_vector:\n",
    "        for S in S_vector:\n",
    "            # Call the SGD optimizer\n",
    "            W = sgd_optimiser(XTrain, yTrain, lmbd, eta, S)\n",
    "            \n",
    "            # Calculate RMSE on validation data \n",
    "            y_pred_val = np.dot(XVal, W)\n",
    "            rmse_val = np.sqrt(np.mean((y_pred_val - yVal) ** 2))\n",
    "            \n",
    "            # Keep track of the best RMSE and the corresponding hyperparameters IF this combination is better\n",
    "            if rmse_val < best_rmse:\n",
    "                best_rmse = rmse_val\n",
    "                optimal_params = (lmbd, eta, S)\n",
    "                optimal_w = W\n",
    "\n",
    "# Ouput the best hyperparameters and the lowest RMSE\n",
    "print(f\"Best Parameters: Lambda = {optimal_params[0]}, Eta = {optimal_params[1]}, S = {optimal_params[2]}\")\n",
    "print(f\"Best RMSE: {best_rmse}\")\n",
    "\n",
    "# Save the optimal values of lambda, eta and S (print them for our reference)\n",
    "lmbd = optimal_params[0]\n",
    "eta = optimal_params[1]\n",
    "S = optimal_params[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and results reporting (5 marks)\n",
    "\n",
    "We now know the best model, according to the validation data. We will now put together the training data and the validation data and perform the preprocessing as before, this is, impute the missing values and scale the inputs. We will train the model again using the minibatch stochastic gradient descent and finally compute the RMSE over the test data. You should do the following:\n",
    "\n",
    "* **Prepare the data:** In this question we will use the test data. First, combine the original training and validation data and standardize again using the mean and std. from this combined data. Save the values from this preprocessing step. Use the saved values to preprocess the test data, similarly to how we used the values from training data to preprocess the validation data. Before training and inference, don't forget to add the column of ones to the data to create the design matrix.\n",
    "\n",
    "* **Re-train your model** on the full training set, using the optimal values of $\\lambda$, $\\eta$ and $S$.\n",
    "\n",
    "* **Run your model** on the test data using the optimal values of of $\\lambda$, $\\eta$ and $S$, and report the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training and validation data into a single final training set\n",
    "data_training_full = pd.concat([data_training, data_val])\n",
    "labels_training_full = pd.concat([labels_training, labels_val])\n",
    "\n",
    "# Standardize the full training set\n",
    "training_means = data_training_full.mean()\n",
    "training_stds = data_training_full.std()\n",
    "\n",
    "data_training_full_standardized = (data_training_full - training_means) / training_stds\n",
    "data_training_full_standardized['is_weekday'] = data_training_full['is_weekday']  \n",
    "\n",
    "# Create the new design matrix and target vector for the full training set\n",
    "XTrain_full = np.concatenate((np.ones((data_training_full_standardized.shape[0], 1)), \n",
    "                              data_training_full_standardized.values), axis=1)\n",
    "yTrain_full = np.reshape(labels_training_full.values, (labels_training_full.shape[0], 1))\n",
    "\n",
    "# Preprocess the test data \n",
    "data_test_standardized = (data_test - training_means) / training_stds\n",
    "data_test_standardized['is_weekday'] = data_test['is_weekday']  \n",
    "\n",
    "# Create the design matrix and target vector for the test set\n",
    "XTest = np.concatenate((np.ones((data_test_standardized.shape[0], 1)), \n",
    "                        data_test_standardized.values), axis=1)\n",
    "yTest = np.reshape(labels_test.values, (labels_test.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the test set: 92.29913721000617\n"
     ]
    }
   ],
   "source": [
    "# Train the regularised linear model and compute the RMSE for the values of lambda, eta and S over the test data\n",
    "optimal_w = sgd_optimiser(XTrain_full, yTrain_full, lmbd, eta, S)\n",
    "\n",
    "# Make predictions on the test set using the optimized weights\n",
    "y_pred_test = np.dot(XTest, optimal_w)\n",
    "\n",
    "# Compute the RMSE on the test set\n",
    "rmse_test = np.sqrt(np.mean((y_pred_test - yTest) ** 2))\n",
    "print(f\"RMSE on the test set: {rmse_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
